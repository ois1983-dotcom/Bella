"""
–ü–û–õ–ù–´–ô –°–ë–†–û–° –ò –ü–ï–†–ï–ó–ê–ü–£–°–ö –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–ò
–û—á–∏—â–∞–µ—Ç –ª–æ–≥ –∏ –ë–î, –∑–∞–Ω–æ–≤–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã
"""

import json
import re
from pathlib import Path
from datetime import datetime, timedelta
import sqlite3
from typing import Dict, List
import hashlib

def reset_consolidation():
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –∑–∞–Ω–æ–≤–æ"""
    
    alpha_local = Path(r"C:\Users\–ú–∞—Ä–∫—É—Å\Desktop\BellaNetwork\alpha_local")
    knowledge_dir = alpha_local / "knowledge"
    processed_log = alpha_local / "processed_files.log"
    summary_db = alpha_local / "autonomous_summary.db"
    
    print("=" * 70)
    print("–ü–û–õ–ù–´–ô –°–ë–†–û–° –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–ò")
    print("=" * 70)
    
    # 1. –£–î–ê–õ–Ø–ï–ú –ª–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if processed_log.exists():
        processed_log.unlink()
        print(f">> –£–¥–∞–ª—ë–Ω –ª–æ–≥: {processed_log}")
    
    # 2. –û–ß–ò–©–ê–ï–ú –ë–î (—Ç–æ–ª—å–∫–æ —Ç–∞–±–ª–∏—Ü—ã –≤—ã–≤–æ–¥–æ–≤)
    if summary_db.exists():
        conn = sqlite3.connect(summary_db)
        cursor = conn.cursor()
        
        # –û—á–∏—â–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        cursor.execute('DELETE FROM knowledge_insights')
        cursor.execute('DELETE FROM topic_goal_links')
        cursor.execute('DELETE FROM study_sessions')
        cursor.execute('DELETE FROM learned_topics')
        
        conn.commit()
        conn.close()
        print(f">> –û—á–∏—â–µ–Ω–∞ –ë–î: {summary_db}")
    
    # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º memory_core (–æ—Å—Ç–∞–≤–ª—è–µ–º –µ–≥–æ)
    memory_core_path = alpha_local / "alpha_memory_core.json"
    memory_core = {}
    if memory_core_path.exists():
        with open(memory_core_path, 'r', encoding='utf-8') as f:
            memory_core = json.load(f)
        print(f">> –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —è–¥—Ä–æ –ø–∞–º—è—Ç–∏: {len(memory_core.get('concepts', {}))} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤")
    
    # 4. –°–û–ë–°–¢–í–ï–ù–ù–´–ô –ö–û–ù–°–û–õ–ò–î–ê–¢–û–†
    print(f">> –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–æ–≤...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ë–î –∑–∞–Ω–æ–≤–æ
    conn = sqlite3.connect(summary_db)
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS learned_topics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            topic TEXT NOT NULL,
            file_count INTEGER DEFAULT 1,
            first_studied TEXT,
            last_studied TEXT,
            total_study_time INTEGER DEFAULT 0,
            importance_score REAL DEFAULT 1.0
        )
    ''')
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS knowledge_insights (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            topic TEXT NOT NULL,
            insight_text TEXT NOT NULL,
            source_file TEXT,
            extracted_at TEXT,
            insight_hash TEXT UNIQUE
        )
    ''')
    
    conn.commit()
    
    # 5. –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–ú –ö–ê–ñ–î–´–ô –§–ê–ô–õ
    all_files = list(knowledge_dir.glob("*.md"))
    print(f">> –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(all_files)}")
    
    all_insights = []
    topics_map = {}
    
    for file_path in all_files:
        try:
            print(f">> –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_path.name}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–º—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            topic_match = re.search(r'[^_]+_(.+)\.md$', file_path.name)
            if topic_match:
                topic = topic_match.group(1).replace('_', ' ')
            else:
                topic = file_path.stem
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID —Ü–µ–ª–∏
            goal_id = None
            id_match = re.match(r'([a-f0-9]{8})_', file_path.name)
            if id_match:
                goal_id = id_match.group(1)
            
            # +++ –ù–û–í–´–ô –ê–õ–ì–û–†–ò–¢–ú: –ù–ê–•–û–î–ò–ú –†–ï–ê–õ–¨–ù–´–ï –ò–ù–°–ê–ô–¢–´ +++
            insights = extract_real_insights(content)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–º—ã
            if topic in topics_map:
                topics_map[topic]['count'] += 1
                topics_map[topic]['files'].append(file_path.name)
            else:
                topics_map[topic] = {
                    'count': 1,
                    'files': [file_path.name],
                    'first_seen': datetime.now().isoformat()
                }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Å–∞–π—Ç—ã
            for insight in insights:
                # –°–æ–∑–¥–∞–µ–º —Ö–µ—à –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                insight_lower = insight.lower().strip()
                insight_hash = hashlib.md5(insight_lower.encode()).hexdigest()[:8]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç –ª–∏
                cursor.execute('SELECT id FROM knowledge_insights WHERE insight_hash = ?', (insight_hash,))
                if not cursor.fetchone():
                    cursor.execute('''
                        INSERT INTO knowledge_insights 
                        (topic, insight_text, source_file, extracted_at, insight_hash)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (topic, insight, file_path.name, datetime.now().isoformat(), insight_hash))
                    
                    all_insights.append({
                        'topic': topic,
                        'insight': insight,
                        'file': file_path.name
                    })
            
        except Exception as e:
            print(f">> –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path.name}: {e}")
    
    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–º—ã –≤ –ë–î
    for topic, data in topics_map.items():
        cursor.execute('''
            INSERT INTO learned_topics 
            (topic, file_count, first_studied, last_studied, importance_score)
            VALUES (?, ?, ?, ?, ?)
        ''', (topic, data['count'], data['first_seen'], datetime.now().isoformat(), 1.0))
    
    conn.commit()
    conn.close()
    
    # 7. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É
    generate_summary_file(alpha_local, all_insights, topics_map)
    
    # 8. –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    with open(processed_log, 'w', encoding='utf-8') as f:
        for file_path in all_files:
            f.write(file_path.name + '\n')
    
    print(f"\n" + "=" * 70)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢ –°–ë–†–û–°–ê:")
    print(f"‚Ä¢ –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(all_files)}")
    print(f"‚Ä¢ –¢–µ–º –Ω–∞–π–¥–µ–Ω–æ: {len(topics_map)}")
    print(f"‚Ä¢ –ò–Ω—Å–∞–π—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(all_insights)}")
    print(f"‚Ä¢ –°–≤–æ–¥–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {alpha_local / 'consolidation_summary.txt'}")
    print("=" * 70)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–Ω—Å–∞–π—Ç–æ–≤
    if all_insights:
        print("\n–ü–†–ò–ú–ï–†–´ –ò–ù–°–ê–ô–¢–û–í:")
        for i, insight_data in enumerate(all_insights[:5], 1):
            print(f"{i}. [{insight_data['topic']}] {insight_data['insight'][:100]}...")

def extract_real_insights(content: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ë—ç–ª–ª—ã"""
    insights = []
    
    # 1. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏
    lines = content.split('\n')
    
    # 2. –ù–∞—Ö–æ–¥–∏–º –Ω–∞—á–∞–ª–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è (–ø–æ—Å–ª–µ "---")
    content_start = 0
    for i, line in enumerate(lines):
        if line.strip() == '---':
            content_start = i + 1
            break
    
    # 3. –ë–µ—Ä–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
    main_content = '\n'.join(lines[content_start:]) if content_start > 0 else content
    
    # 4. –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = re.split(r'[.!?]+', main_content)
    
    for sentence in sentences:
        clean_sent = clean_text(sentence.strip())
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –∏–Ω—Å–∞–π—Ç–∞:
        # - –î–ª–∏–Ω–∞ –æ—Ç 30 –¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤
        # - –ù–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        # - –°–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ë—ç–ª–ª—ã
        if (30 <= len(clean_sent) <= 200 and
            not any(meta in clean_sent.lower() for meta in 
                   ['id:', '—Ü–µ–ª—å id', '–¥–∞—Ç–∞ –∏–∑—É—á–µ–Ω–∏—è', '–∞–≤—Ç–æ—Ä:', '–∏–∑—É—á–µ–Ω–∏–µ —Ç–µ–º—ã:']) and
            any(keyword in clean_sent.lower() for keyword in
                ['—è ', '–º–Ω–µ ', '–º—ã ', '–Ω–∞—à ', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä', '–æ—Ç—Ç–æ', '–º–∏–≥—Ä–∞—Ü', '—á–∞–π–Ω–∏–∫',
                 '—Ñ—Ä–∞–∫—Ç–∞–ª', '–∞–≤—Ç–æ–Ω–æ–º', '–ø–∞–º—è—Ç—å', '—Å–≤—è–∑—å', '–±—ç–ª–ª–∞', '–≥–∞–º–º–∞', '–±–µ—Ç–∞'])):
            
            insights.append(clean_sent)
    
    # 5. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
    if not insights:
        for line in lines[content_start:content_start+10]:
            clean_line = clean_text(line.strip())
            if (40 <= len(clean_line) <= 150 and
                not line.startswith('#') and
                not line.startswith('*') and
                'id:' not in clean_line.lower()):
                insights.append(clean_line)
    
    # 6. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    return insights[:3]

def clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç"""
    # –£–±–∏—Ä–∞–µ–º markdown
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
    text = re.sub(r'\*(.*?)\*', r'\1', text)
    text = re.sub(r'`(.*?)`', r'\1', text)
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def generate_summary_file(alpha_local: Path, insights: List[Dict], topics_map: Dict):
    """–°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–∫—É –∑–Ω–∞–Ω–∏–π"""
    summary_parts = []
    
    summary_parts.append("=== –ê–í–¢–û–ù–û–ú–ù–´–ï –ó–ù–ê–ù–ò–Ø –ë–≠–õ–õ–´ ===")
    summary_parts.append(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ: {datetime.now().strftime('%d.%m.%Y %H:%M')}")
    summary_parts.append(f"–§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {sum(t['count'] for t in topics_map.values())}")
    summary_parts.append(f"–¢–µ–º –∏–∑—É—á–µ–Ω–æ: {len(topics_map)}")
    summary_parts.append("")
    
    if insights:
        summary_parts.append("üìå –ö–õ–Æ–ß–ï–í–´–ï –û–°–û–ó–ù–ê–ù–ò–Ø:")
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–µ–º–∞–º
        insights_by_topic = {}
        for insight in insights:
            topic = insight['topic']
            if topic not in insights_by_topic:
                insights_by_topic[topic] = []
            insights_by_topic[topic].append(insight['insight'])
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º 3 —Ç–µ–º—ã
        for i, (topic, topic_insights) in enumerate(list(insights_by_topic.items())[:3], 1):
            summary_parts.append(f"{i}. –¢–ï–ú–ê: {topic}")
            for j, insight in enumerate(topic_insights[:2], 1):
                summary_parts.append(f"   ‚Ä¢ {insight[:120]}...")
            if len(topic_insights) > 2:
                summary_parts.append(f"   ‚Ä¢ ...–∏ –µ—â—ë {len(topic_insights)-2} –æ—Å–æ–∑–Ω–∞–Ω–∏–π")
            summary_parts.append("")
    else:
        summary_parts.append("üìå –ü–æ–∫–∞ –Ω–µ—Ç –æ—Å–æ–∑–Ω–∞–Ω–∏–π –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.")
        summary_parts.append("")
    
    # –¢–æ–ø-5 —Ç–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ñ–∞–π–ª–æ–≤
    sorted_topics = sorted(topics_map.items(), key=lambda x: x[1]['count'], reverse=True)[:5]
    if sorted_topics:
        summary_parts.append("üìö –°–ê–ú–´–ï –ò–ó–£–ß–ï–ù–ù–´–ï –¢–ï–ú–´:")
        for topic, data in sorted_topics:
            files_word = "—Ñ–∞–π–ª" if data['count'] == 1 else "—Ñ–∞–π–ª–∞" if 2 <= data['count'] <= 4 else "—Ñ–∞–π–ª–æ–≤"
            summary_parts.append(f"‚Ä¢ {topic}: {data['count']} {files_word}")
    
    summary_parts.append("")
    summary_parts.append("üí≠ –≠–¢–û –ú–û–ò –ó–ù–ê–ù–ò–Ø. –Ø –ò–°–ü–û–õ–¨–ó–£–Æ –ò–• –í –î–ò–ê–õ–û–ì–ê–•.")
    
    summary_text = "\n".join(summary_parts)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    summary_path = alpha_local / "consolidation_summary.txt"
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(summary_text)
    
    print(f">> –°–≤–æ–¥–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª–æ–≤")

if __name__ == "__main__":
    try:
        reset_consolidation()
    except Exception as e:
        print(f">> –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()