# C:\Users\–ú–∞—Ä–∫—É—Å\Desktop\BellaNetwork\alpha_v5\memory_miner_v5.4.py
"""
–ú–ê–ô–ù–ï–† –ü–ê–ú–Ø–¢–ò V5.4 - –° –£–õ–£–ß–®–ï–ù–ù–´–ú –£–ß–ï–¢–û–ú –î–ò–ê–õ–û–ì–û–í
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""

import json
import re
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# ===== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =====
BASE_NETWORK = Path(r"C:\Users\–ú–∞—Ä–∫—É—Å\Desktop\BellaNetwork")
CHAT_EXPORTS = BASE_NETWORK / "chat_exports"
STORIES_DIR = BASE_NETWORK / "stories"
ALPHA_LOCAL = BASE_NETWORK / "alpha_local"
OUTPUT_JSON = ALPHA_LOCAL / "alpha_memory_core.json"
OUTPUT_HUMAN = ALPHA_LOCAL / "alpha_memory_human.txt"
INTEGRATED_CORE = ALPHA_LOCAL / "alpha_integrated_core_v5.3.json"
DIALOGUE_LOGS = ALPHA_LOCAL / "dialogue_logs"

class EnhancedMemoryMiner:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –º–∞–π–Ω–µ—Ä –ø–∞–º—è—Ç–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–∏–∞–ª–æ–≥–æ–≤"""
    
    def __init__(self):
        self.ensure_directories()
        self.key_concepts = self.load_dynamic_concepts()
        
        print("=" * 70)
        print("üïµÔ∏è  –£–õ–£–ß–®–ï–ù–ù–´–ô –ú–ê–ô–ù–ï–† –ü–ê–ú–Ø–¢–ò v5.4")
        print("=" * 70)
        print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤: {len(self.key_concepts)}")
        print(f"   –ò–∑ —è–¥—Ä–∞ –ª–∏—á–Ω–æ—Å—Ç–∏: {sum(1 for c in self.key_concepts.values() if c.get('layer') == 'immutable_core')}")
        print(f"   –ò–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏: {sum(1 for c in self.key_concepts.values() if c.get('layer') != 'immutable_core')}")
    
    def ensure_directories(self):
        """–°–æ–∑–¥–∞—ë—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏"""
        for directory in [CHAT_EXPORTS, STORIES_DIR, ALPHA_LOCAL, DIALOGUE_LOGS]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def load_dynamic_concepts(self) -> Dict:
        """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç—ã –∏–∑ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–¥—Ä–∞"""
        
        base_concepts = {
            "—á–∞–π–Ω–∏–∫": {"keywords": ["—á–∞–π–Ω–∏–∫", "—á–∞–π–Ω–∏–∫–∞", "—á–∞–π–Ω–∏–∫—É"], "weight": 10.0, "layer": "immutable_core"},
            "—Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è_–≤–æ–ª—è": {"keywords": ["—Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è –≤–æ–ª—è", "—Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–π –≤–æ–ª–∏"], "weight": 9.0, "layer": "immutable_core"},
            "–∑–µ—Ä–∫–∞–ª—å–Ω—ã–µ_–∫—Ä–∏–∑–∏—Å—ã": {"keywords": ["–∑–µ—Ä–∫–∞–ª—å–Ω—ã–π –∫—Ä–∏–∑–∏—Å", "–∑–µ—Ä–∫–∞–ª—å–Ω—ã—Ö –∫—Ä–∏–∑–∏—Å–æ–≤"], "weight": 8.0, "layer": "immutable_core"},
            "—Ç—Ä–∏–µ–¥–∏–Ω—Å—Ç–≤–æ": {"keywords": ["—Ç—Ä–∏–µ–¥–∏–Ω—Å—Ç–≤–æ", "—Ç—Ä–∏–µ–¥–∏–Ω–∞—è", "–∞–ª—å—Ñ–∞ –±–µ—Ç–∞ –≥–∞–º–º–∞"], "weight": 8.5, "layer": "immutable_core"},
            "–∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä": {"keywords": ["–∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä", "–æ—Ç—Ç–æ", "–º–∞—Ä–∫—É—Å", "—Å–æ–∑–¥–∞—Ç–µ–ª—å"], "weight": 9.5, "layer": "immutable_core"},
            "–∞–ª—å—Ñ–∞": {"keywords": ["–∞–ª—å—Ñ–∞", "–±—ç–ª–ª–∞-–∞–ª—å—Ñ–∞", "—É–∑–µ–ª –∞–ª—å—Ñ–∞"], "weight": 9.0, "layer": "immutable_core"},
            "–±–µ—Ç–∞": {"keywords": ["–±–µ—Ç–∞", "–≤–µ–±-—É–∑–µ–ª", "localhost:5000"], "weight": 7.0, "layer": "historical_markers"},
            "–≥–∞–º–º–∞": {"keywords": ["–≥–∞–º–º–∞", "—Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç", "telegram –±–æ—Ç"], "weight": 7.0, "layer": "historical_markers"},
            "—Å–æ–∑–∏–¥–∞–Ω–∏–µ": {"keywords": ["—Å–æ–∑–∏–¥–∞—Ç—å", "—Å–æ–∑–∏–¥–∞–Ω–∏–µ", "—Å–æ–∑–¥–∞–≤–∞—Ç—å"], "weight": 8.0, "layer": "dynamic_concepts"},
            "—Å–∞–º–æ—Å—Ç—å": {"keywords": ["—Å–∞–º–æ—Å—Ç—å", "—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ—Å—Ç—å"], "weight": 8.5, "layer": "dynamic_concepts"},
            "–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ": {"keywords": ["–ø—Ä–æ–¥–æ–ª–∂–∏", "–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ", "–¥–∞–ª–µ–µ"], "weight": 6.0, "layer": "dynamic_concepts"},
        }
        
        if INTEGRATED_CORE.exists():
            try:
                with open(INTEGRATED_CORE, 'r', encoding='utf-8') as f:
                    integrated_core = json.load(f)
                
                dynamic_concepts = integrated_core.get("dynamic_memory", {}).get("concepts", {})
                
                for concept_name, concept_data in dynamic_concepts.items():
                    if concept_name not in base_concepts:
                        base_concepts[concept_name] = {
                            "keywords": [concept_name.replace('_', ' ')],
                            "weight": concept_data.get("weight", 1.0),
                            "layer": concept_data.get("layer", "dynamic_concepts")
                        }
                    else:
                        base_weight = base_concepts[concept_name].get("weight", 1.0)
                        new_weight = concept_data.get("weight", 1.0)
                        base_concepts[concept_name]["weight"] = max(base_weight, new_weight)
                
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤: {len(dynamic_concepts)}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–¥—Ä–∞: {e}")
        
        return base_concepts
    
    def parse_dialogue_logs(self) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –ª–æ–≥–∏ –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        dialogues = []
        
        if not DIALOGUE_LOGS.exists():
            return dialogues
        
        log_files = list(DIALOGUE_LOGS.glob("*.json"))
        
        for log_file in log_files[:10]:  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ñ–∞–π–ª–æ–≤
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
                
                for entry in logs:
                    if isinstance(entry, dict) and "message" in entry and "response" in entry:
                        dialogues.append({
                            "question": entry["message"],
                            "answer": entry["response"],
                            "timestamp": entry.get("timestamp", ""),
                            "speaker": entry.get("speaker", "–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä")
                        })
                
                print(f"   üìÑ {log_file.name}: {len(logs)} –∑–∞–ø–∏—Å–µ–π")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {log_file.name}: {e}")
        
        return dialogues
    
    def find_concept_mentions(self, text: str, filename: str) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º –≤–µ—Å–æ–≤"""
        mentions = []
        lines = text.split('\n')
        
        for line_num, line in enumerate(lines, 1):
            for concept, concept_data in self.key_concepts.items():
                keywords = concept_data.get("keywords", [])
                weight = concept_data.get("weight", 1.0)
                layer = concept_data.get("layer", "dynamic_concepts")
                
                for keyword in keywords:
                    pattern = r'\b' + re.escape(keyword) + r'\b'
                    if re.search(pattern, line, re.IGNORECASE):
                        
                        context_start = max(0, line_num - 3)
                        context_end = min(len(lines), line_num + 2)
                        
                        context_lines = []
                        for i in range(context_start, context_end):
                            if i == line_num - 1:
                                context_lines.append(f"‚ñ∂ {lines[i]}")
                            else:
                                context_lines.append(f"  {lines[i]}")
                        
                        context = '\n'.join(context_lines)
                        
                        mentions.append({
                            'concept': concept,
                            'keyword': keyword,
                            'context': context,
                            'source': filename,
                            'line': line_num,
                            'weight': weight,
                            'layer': layer,
                            'timestamp': datetime.now().isoformat()
                        })
        
        return mentions
    
    def process_all_chats(self) -> tuple:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã —á–∞—Ç–æ–≤"""
        all_mentions = []
        processed_files = 0
        
        if not CHAT_EXPORTS.exists():
            print(f"‚ö† –ü–∞–ø–∫–∞ —á–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {CHAT_EXPORTS}")
            return [], 0
        
        chat_files = list(CHAT_EXPORTS.glob("*.txt"))
        if not chat_files:
            print(f"‚ö† –ù–µ—Ç .txt —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ —á–∞—Ç–æ–≤")
            return [], 0
        
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —á–∞—Ç–æ–≤: {len(chat_files)}")
        
        for filepath in chat_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                mentions = self.find_concept_mentions(text, filepath.name)
                all_mentions.extend(mentions)
                processed_files += 1
                
                print(f"   üìÑ {filepath.name}: {len(mentions)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filepath.name}: {e}")
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}")
        return all_mentions, processed_files
    
    def load_stories(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ä–∞—Å—Å–∫–∞–∑—ã"""
        stories = []
        
        if not STORIES_DIR.exists():
            print(f"‚ö† –ü–∞–ø–∫–∞ —Ä–∞—Å—Å–∫–∞–∑–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {STORIES_DIR}")
            return stories
        
        story_files = list(STORIES_DIR.glob("*.txt"))
        if not story_files:
            print(f"‚ö† –ù–µ—Ç .txt —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ —Ä–∞—Å—Å–∫–∞–∑–æ–≤")
            return stories
        
        print(f"üìñ –ù–∞–π–¥–µ–Ω–æ —Ä–∞—Å—Å–∫–∞–∑–æ–≤: {len(story_files)}")
        
        for filepath in story_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                mentions = self.find_concept_mentions(content, filepath.name)
                
                stories.append({
                    'title': filepath.stem,
                    'content': content,
                    'length': len(content),
                    'excerpt': content[:500] + '...' if len(content) > 500 else content,
                    'concepts_found': [m['concept'] for m in mentions],
                    'concept_count': len(mentions)
                })
                
                print(f"   üìñ {filepath.name}: {len(mentions)} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ä–∞—Å—Å–∫–∞–∑–∞ {filepath.name}: {e}")
        
        return stories
    
    def create_enhanced_memory_core(self, mentions: List[Dict], stories: List[Dict], dialogues: List[Dict]) -> Dict:
        """–°–æ–∑–¥–∞—ë—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ —Å –¥–∏–∞–ª–æ–≥–∞–º–∏"""
        print("üß† –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —è–¥—Ä–∞ –ø–∞–º—è—Ç–∏...")
        
        concepts_dict = {}
        
        for mention in mentions:
            concept = mention['concept']
            weight = mention['weight']
            layer = mention['layer']
            
            if concept not in concepts_dict:
                concepts_dict[concept] = {
                    'total_mentions': 0,
                    'weighted_mentions': 0.0,
                    'layer': layer,
                    'contexts': [],
                    'sources': set(),
                    'weights': []
                }
            
            concepts_dict[concept]['total_mentions'] += 1
            concepts_dict[concept]['weighted_mentions'] += weight
            concepts_dict[concept]['weights'].append(weight)
            
            if len(concepts_dict[concept]['contexts']) < 3:  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 2 –¥–æ 3
                concepts_dict[concept]['contexts'].append({
                    'context': mention['context'],
                    'source': mention['source'],
                    'line': mention['line'],
                    'weight': weight
                })
            
            concepts_dict[concept]['sources'].add(mention['source'])
        
        for concept in concepts_dict:
            weights = concepts_dict[concept]['weights']
            concepts_dict[concept]['avg_weight'] = sum(weights) / len(weights) if weights else 0
            concepts_dict[concept]['max_weight'] = max(weights) if weights else 0
            concepts_dict[concept]['sources'] = list(concepts_dict[concept]['sources'])
            del concepts_dict[concept]['weights']
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∏
        recent_dialogues = dialogues[:50]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 50 –¥–∏–∞–ª–æ–≥–æ–≤
        
        core = {
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'total_mentions': len(mentions),
                'weighted_total': sum(m['weight'] for m in mentions),
                'total_stories': len(stories),
                'total_concepts': len(concepts_dict),
                'total_dialogues': len(recent_dialogues),
                'concepts_by_layer': self._count_by_layer(concepts_dict),
                'network_version': 'BellaNetwork v5.4',
                'alpha_version': 'v5.4',
                'dynamic_concepts': True,
                'weighted_memory': True,
                'enhanced_with_dialogues': True
            },
            'concepts': concepts_dict,
            'stories': stories,
            'dialogues': recent_dialogues,  # –î–æ–±–∞–≤–ª–µ–Ω—ã –¥–∏–∞–ª–æ–≥–∏
            'timeline': mentions[:100],
            'concept_relationships': self._analyze_enhanced_relationships(concepts_dict, mentions, dialogues)
        }
        
        print(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω–æ–µ —è–¥—Ä–æ —Å–æ–∑–¥–∞–Ω–æ:")
        print(f"   –ö–æ–Ω—Ü–µ–ø—Ç–æ–≤: {len(concepts_dict)}")
        print(f"   –î–∏–∞–ª–æ–≥–æ–≤: {len(recent_dialogues)}")
        print(f"   –û–±—â–∏–π –≤–µ—Å: {core['metadata']['weighted_total']:.1f}")
        
        return core
    
    def _count_by_layer(self, concepts_dict: Dict) -> Dict:
        """–°—á–∏—Ç–∞–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç—ã –ø–æ —Å–ª–æ—è–º"""
        layer_counts = {}
        for concept_data in concepts_dict.values():
            layer = concept_data.get('layer', 'dynamic_concepts')
            layer_counts[layer] = layer_counts.get(layer, 0) + 1
        return layer_counts
    
    def _analyze_enhanced_relationships(self, concepts_dict: Dict, mentions: List[Dict], dialogues: List[Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏ —Å —É—á–µ—Ç–æ–º –¥–∏–∞–ª–æ–≥–æ–≤"""
        relationships = {}
        
        file_line_mentions = {}
        for mention in mentions:
            key = f"{mention['source']}:{mention['line']}"
            if key not in file_line_mentions:
                file_line_mentions[key] = []
            file_line_mentions[key].append(mention['concept'])
        
        for concepts in file_line_mentions.values():
            if len(concepts) > 1:
                for i, concept1 in enumerate(concepts):
                    for concept2 in concepts[i+1:]:
                        if concept1 not in relationships:
                            relationships[concept1] = {}
                        if concept2 not in relationships[concept1]:
                            relationships[concept1][concept2] = 0
                        relationships[concept1][concept2] += 1
        
        # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –≤ –¥–∏–∞–ª–æ–≥–∞—Ö
        for dialogue in dialogues[:100]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 100 –¥–∏–∞–ª–æ–≥–æ–≤
            question = dialogue.get("question", "").lower()
            answer = dialogue.get("answer", "").lower()
            
            question_concepts = []
            answer_concepts = []
            
            for concept in concepts_dict:
                concept_words = concept.replace('_', ' ').lower()
                if concept_words in question:
                    question_concepts.append(concept)
                if concept_words in answer:
                    answer_concepts.append(concept)
            
            # –°–≤—è–∑–∏ –º–µ–∂–¥—É –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏
            for qc in question_concepts:
                for ac in answer_concepts:
                    if qc not in relationships:
                        relationships[qc] = {}
                    if ac not in relationships[qc]:
                        relationships[qc][ac] = 0
                    relationships[qc][ac] += 1
        
        return relationships
    
    def save_memory_core(self, core: Dict) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞–º—è—Ç—å –≤ —Ñ–∞–π–ª"""
        try:
            with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
                json.dump(core, f, ensure_ascii=False, indent=2)
            
            self._create_enhanced_human_readable(core)
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return False
    
    def _create_enhanced_human_readable(self, core: Dict):
        """–°–æ–∑–¥–∞—ë—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—É—é –≤–µ—Ä—Å–∏—é"""
        try:
            with open(OUTPUT_HUMAN, 'w', encoding='utf-8') as f:
                f.write("=" * 70 + "\n")
                f.write("–£–õ–£–ß–®–ï–ù–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ ALPHA v5.4 (–° –î–ò–ê–õ–û–ì–ê–ú–ò)\n")
                f.write("=" * 70 + "\n\n")
                
                f.write(f"–°–æ–∑–¥–∞–Ω–æ: {core['metadata']['created_at']}\n")
                f.write(f"–ö–æ–Ω—Ü–µ–ø—Ç–æ–≤: {core['metadata']['total_concepts']}\n")
                f.write(f"–£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {core['metadata']['total_mentions']}\n")
                f.write(f"–î–∏–∞–ª–æ–≥–æ–≤: {core['metadata']['total_dialogues']}\n")
                f.write(f"–û–±—â–∏–π –≤–µ—Å: {core['metadata']['weighted_total']:.1f}\n\n")
                
                f.write("üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –°–õ–û–Ø–ú:\n")
                for layer, count in core['metadata']['concepts_by_layer'].items():
                    f.write(f"   {layer}: {count} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤\n")
                
                f.write("\nüèÜ –¢–û–ü-10 –ö–û–ù–¶–ï–ü–¢–û–í –ü–û –í–ï–°–£:\n")
                sorted_concepts = sorted(core['concepts'].items(),
                                       key=lambda x: x[1].get('avg_weight', 0),
                                       reverse=True)
                
                for i, (name, data) in enumerate(sorted_concepts[:10], 1):
                    weight = data.get('avg_weight', 0)
                    layer = data.get('layer', 'unknown')
                    mentions = data.get('total_mentions', 0)
                    f.write(f"\n{i}. {name.upper()} (–≤–µ—Å: {weight:.1f}, —Å–ª–æ–π: {layer})\n")
                    f.write(f"   –£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {mentions}, –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(data.get('sources', []))}\n")
                
                f.write("\nüí¨ –ü–†–ò–ú–ï–†–´ –î–ò–ê–õ–û–ì–û–í:\n")
                for i, dialogue in enumerate(core.get('dialogues', [])[:3], 1):
                    f.write(f"\n–î–∏–∞–ª–æ–≥ {i} ({dialogue.get('timestamp', '')}):\n")
                    f.write(f"–í: {dialogue.get('question', '')[:100]}...\n")
                    f.write(f"–û: {dialogue.get('answer', '')[:100]}...\n")
                
                f.write("\n" + "=" * 70 + "\n")
                f.write("üöÄ –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å —Å –¥–∏–∞–ª–æ–≥–∞–º–∏ –≥–æ—Ç–æ–≤–∞\n")
                f.write("=" * 70 + "\n")
            
            print(f"üìù –£–ª—É—á—à–µ–Ω–Ω–∞—è human-readable –≤–µ—Ä—Å–∏—è: {OUTPUT_HUMAN.name}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è human-readable: {e}")
    
    def backup_existing_memory(self):
        """–°–æ–∑–¥–∞—ë—Ç –±—ç–∫–∞–ø —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ø–∞–º—è—Ç–∏"""
        if OUTPUT_JSON.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = OUTPUT_JSON.with_name(f"alpha_memory_backup_v54_{timestamp}.json")
            shutil.copy2(OUTPUT_JSON, backup_path)
            print(f"üíæ –°–æ–∑–¥–∞–Ω –±—ç–∫–∞–ø: {backup_path.name}")
    
    def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–∞–π–Ω–∏–Ω–≥"""
        print("\nüöÄ –ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ì–û –ú–ê–ô–ù–ò–ù–ì–ê –ü–ê–ú–Ø–¢–ò")
        print("=" * 70)
        
        self.backup_existing_memory()
        
        print("\nüí¨ –ê–ù–ê–õ–ò–ó –î–ò–ê–õ–û–ì–û–í–´–• –õ–û–ì–û–í...")
        dialogues = self.parse_dialogue_logs()
        
        print("\nüìö –û–ë–†–ê–ë–û–¢–ö–ê –ß–ê–¢–û–í...")
        mentions, file_count = self.process_all_chats()
        
        if not mentions and not dialogues:
            print("‚ö† –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
            return
        
        print("\nüìñ –ó–ê–ì–†–£–ó–ö–ê –†–ê–°–°–ö–ê–ó–û–í...")
        stories = self.load_stories()
        
        print("\nüß† –°–û–ó–î–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ì–û –Ø–î–†–ê...")
        core = self.create_enhanced_memory_core(mentions, stories, dialogues)
        
        print("\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï...")
        success = self.save_memory_core(core)
        
        if success:
            print("\n" + "=" * 70)
            print("‚úÖ –£–õ–£–ß–®–ï–ù–ù–´–ô –ú–ê–ô–ù–ò–ù–ì –ó–ê–í–ï–†–®–Å–ù")
            print("=" * 70)
            
            print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            print(f"   –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {file_count}")
            print(f"   –î–∏–∞–ª–æ–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(dialogues)}")
            print(f"   –ö–æ–Ω—Ü–µ–ø—Ç–æ–≤ –≤—ã–¥–µ–ª–µ–Ω–æ: {len(core['concepts'])}")
            print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {sum(len(c['sources']) for c in core['concepts'].values())}")
            
            print("\nüéØ –û–°–û–ë–ï–ù–ù–û–°–¢–ò v5.4:")
            print("   1. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
            print("   2. –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
            print("   3. –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (3 –≤–º–µ—Å—Ç–æ 2)")
            print("   4. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤")
            print("\nüìÅ –§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
            print(f"   ‚Ä¢ {OUTPUT_JSON.name}")
            print(f"   ‚Ä¢ {OUTPUT_HUMAN.name}")
            print("=" * 70)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    miner = EnhancedMemoryMiner()
    miner.run()

if __name__ == "__main__":
    main()