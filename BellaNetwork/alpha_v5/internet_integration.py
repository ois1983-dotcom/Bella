"""
–ú–û–î–£–õ–¨ –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –ò–ù–¢–ï–†–ù–ï–¢–ê –î–õ–Ø ALPHA V5.4
–î–æ—Å—Ç—É–ø –∫ Wikipedia API —á–µ—Ä–µ–∑ wikipedia-api –±–∏–±–ª–∏–æ—Ç–µ–∫—É
–ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ —Ç–µ–º –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""

import wikipediaapi
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import time
import hashlib
import logging
import os

class InternetIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ —á–µ—Ä–µ–∑ Wikipedia API —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º wikipedia-api –±–∏–±–ª–∏–æ—Ç–µ–∫–∏"""
    
    def __init__(self, alpha_local_path: Path):
        self.alpha_local = Path(alpha_local_path)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Wikipedia API —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º User-Agent
        user_agent = "AlphaBellaNetwork/1.0 (https://localhost:5001; contact@bellanetwork.local) Python/3.9"
        self.wiki_wiki = wikipediaapi.Wikipedia(
            user_agent=user_agent,
            language='ru',  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
            extract_format=wikipediaapi.ExtractFormat.WIKI
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º User-Agent –¥–ª—è –ø—Ä—è–º—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.user_agent = user_agent
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π API endpoint –¥–ª—è –ø—Ä—è–º—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ 
        self.direct_api_url = "https://ru.wikipedia.org/w/api.php"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.timeout = 30
        self.max_results = 5
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
        self.log_path = self.alpha_local / "internet_requests_log.json"
        self.knowledge_cache_path = self.alpha_local / "internet_knowledge_cache.json"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ 
        self.logger = logging.getLogger('internet_integration')
        self.logger.setLevel(logging.INFO)
        
        print(">> üåê –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Wikipedia API...")
        print(f">>   User-Agent: {user_agent}")
        print(f">>   –Ø–∑—ã–∫: ru (—Ä—É—Å—Å–∫–∏–π)")
        print(f">>   API Endpoint: {self.direct_api_url}")
    
    def is_internet_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –∏ Wikipedia API"""
        try:
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ—Å—Ç—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É
            test_page = self.wiki_wiki.page("–í–∏–∫–∏–ø–µ–¥–∏—è")
            return test_page.exists()
        except Exception as e:
            print(f">> ‚ö†Ô∏è Wikipedia API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É): {e}")
            
            # Fallback: –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º User-Agent
            try:
                import requests
                headers = {
                    'User-Agent': self.user_agent,
                    'Accept': 'application/json'
                }
                response = requests.get(
                    f"{self.direct_api_url}?action=query&format=json&prop=info&titles=–í–∏–∫–∏–ø–µ–¥–∏—è",
                    timeout=10,
                    headers=headers
                )
                return response.status_code == 200
            except Exception as e2:
                print(f">> ‚ö†Ô∏è Wikipedia API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–ø—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å): {e2}")
                return False
    
    def search_wikipedia(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ Wikipedia —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É wikipedia-api"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É
            search_results = []
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞–ø—Ä—è–º—É—é, –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø–æ—Ö–æ–∂ –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            direct_page = self.wiki_wiki.page(query)
            if direct_page.exists():
                search_results.append({
                    "title": direct_page.title,
                    "pageid": "direct",
                    "snippet": direct_page.summary[:200] if direct_page.summary else "",
                    "exists": True
                })
            
            # –î–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º API –Ω–∞–ø—Ä—è–º—É—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º User-Agent
            import requests
            encoded_query = requests.utils.quote(query)
            url = f"{self.direct_api_url}?action=query&list=search&srsearch={encoded_query}&utf8=1&format=json&srlimit={self.max_results}&srwhat=text"
            
            headers = {
                'User-Agent': self.user_agent,
                'Accept': 'application/json'
            }
            
            response = requests.get(url, timeout=self.timeout, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            api_results = data.get("query", {}).get("search", [])
            
            for result in api_results:
                search_results.append({
                    "title": result.get("title", ""),
                    "pageid": result.get("pageid", ""),
                    "snippet": self._clean_html(result.get("snippet", "")),
                    "timestamp": result.get("timestamp", ""),
                    "wordcount": result.get("wordcount", 0),
                    "exists": True
                })
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            self._log_request(query, "search", success=True, results_count=len(search_results))
            
            return search_results
            
        except Exception as e:
            print(f">> ‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Wikipedia: {e}")
            self._log_request(query, "search", success=False, error=str(e))
            return []
    
    def get_wikipedia_page(self, page_title: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Wikipedia —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º wikipedia-api –±–∏–±–ª–∏–æ—Ç–µ–∫—É
            page = self.wiki_wiki.page(page_title)
            
            if not page.exists():
                print(f">> ‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ '{page_title}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return None
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã 
            full_text = page.text
            
            content = {
                "title": page.title,
                "summary": page.summary,
                "full_text": full_text,
                "fullurl": page.fullurl,
                "canonicalurl": page.canonicalurl,
                "language": 'ru',
                "timestamp": datetime.now().isoformat(),
                "text_length": len(full_text),
                "sections_count": len(page.sections) if hasattr(page, 'sections') else 0
            }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–∫—Ü–∏–∏, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã 
            if hasattr(page, 'sections') and page.sections:
                sections_data = []
                for section in page.sections:
                    sections_data.append({
                        "title": section.title,
                        "text": section.text[:500] if section.text else "",
                        "level": section.level
                    })
                content["sections"] = sections_data[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            self._log_request(page_title, "get_page", success=True, content_length=len(full_text))
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self._cache_knowledge(page_title, content)
            
            return content
            
        except Exception as e:
            print(f">> ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã Wikipedia: {e}")
            self._log_request(page_title, "get_page", success=False, error=str(e))
            
            # Fallback: –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–µ API
            return self._get_page_direct_api(page_title)
    
    def _get_page_direct_api(self, page_title: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–µ API (fallback –º–µ—Ç–æ–¥)"""
        try:
            import requests
            encoded_title = requests.utils.quote(page_title)
            url = f"{self.direct_api_url}?action=query&prop=extracts|info&explaintext=1&exintro=1&titles={encoded_title}&format=json&inprop=url"
            
            headers = {
                'User-Agent': self.user_agent,
                'Accept': 'application/json'
            }
            
            response = requests.get(url, timeout=self.timeout, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            pages = data.get("query", {}).get("pages", {})
            
            if not pages or '-1' in pages:
                return None
            
            page_id = list(pages.keys())[0]
            page_data = pages[page_id]
            
            content = {
                "title": page_data.get("title", page_title),
                "summary": page_data.get("extract", ""),
                "full_text": page_data.get("extract", ""),
                "fullurl": page_data.get("fullurl", f"https://ru.wikipedia.org/wiki/{encoded_title}"),
                "language": 'ru',
                "timestamp": datetime.now().isoformat(),
                "text_length": len(page_data.get("extract", "")),
                "direct_api": True  # –§–ª–∞–≥, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –ø—Ä—è–º–æ–µ API
            }
            
            return content
            
        except Exception as e:
            print(f">> ‚ùå –û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–≥–æ API: {e}")
            return None
    
    def search_and_learn_topic(self, topic: str) -> Dict:
        """–ü–æ–∏—Å–∫ –∏ –∏–∑—É—á–µ–Ω–∏–µ —Ç–µ–º—ã –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"""
        print(f">> üåê –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {topic}")
        
        if not self.is_internet_available():
            return {
                "success": False,
                "error": "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                "topic": topic,
                "timestamp": datetime.now().isoformat()
            }
        
        try:
            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cached = self.get_cached_knowledge(topic)
            if cached:
                print(f">> üìö –ò—Å–ø–æ–ª—å–∑—É—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è: {topic}")
                return {
                    "success": True,
                    "topic": topic,
                    "cached": True,
                    "page_title": cached.get("content", {}).get("title", ""),
                    "extract_preview": cached.get("content", {}).get("summary", "")[:500],
                    "timestamp": cached.get("cached_at", ""),
                    "source": "cache"
                }
            
            # 2. –ò—â–µ–º —Å—Ç–∞—Ç—å–∏
            search_results = self.search_wikipedia(topic)
            
            if not search_results:
                print(f">> ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Ç–µ–º—ã: {topic}")
                return {
                    "success": False,
                    "error": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
                    "topic": topic,
                    "timestamp": datetime.now().isoformat()
                }
            
            # 3. –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            best_result = search_results[0]
            page_content = self.get_wikipedia_page(best_result["title"])
            
            if not page_content:
                return {
                    "success": False,
                    "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É",
                    "topic": topic,
                    "search_results": [r["title"] for r in search_results[:3]],
                    "timestamp": datetime.now().isoformat()
                }
            
            # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã
            extract = page_content.get("summary", "") or page_content.get("full_text", "")
            key_facts = self._extract_key_facts(extract, topic)
            
            # 5. –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞–Ω–∏—è –¥–ª—è Alpha
            formatted_knowledge = self._format_for_alpha(
                topic, 
                page_content, 
                key_facts, 
                search_results
            )
            
            result = {
                "success": True,
                "topic": topic,
                "page_title": page_content["title"],
                "url": page_content.get("fullurl", ""),
                "extract_preview": extract[:500] + "..." if len(extract) > 500 else extract,
                "key_facts": key_facts,
                "formatted_knowledge": formatted_knowledge,
                "search_results_count": len(search_results),
                "content_length": len(extract),
                "timestamp": datetime.now().isoformat(),
                "source": "wikipedia",
                "cached": False
            }
            
            print(f">> ‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {page_content['title']} ({len(extract)} —Å–∏–º–≤–æ–ª–æ–≤)")
            print(f">>    –ö–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤: {len(key_facts)}")
            print(f">>    URL: {page_content.get('fullurl', '')}")
            
            return result
            
        except Exception as e:
            print(f">> ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑—É—á–µ–Ω–∏–∏ —Ç–µ–º—ã –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞: {e}")
            import traceback
            print(f"–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()[:200]}")
            
            return {
                "success": False,
                "error": str(e),
                "topic": topic,
                "timestamp": datetime.now().isoformat()
            }
    
    def _extract_key_facts(self, text: str, topic: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        facts = []
        
        keywords = [k.lower() for k in topic.split() if len(k) > 3]
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 25 or len(sentence) > 400:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            sentence_lower = sentence.lower()
            relevance_score = sum(1 for word in keywords if word in sentence_lower)
            
            # –¢–∞–∫–∂–µ –∏—â–µ–º —Ü–∏—Ñ—Ä—ã –∏ –≤–∞–∂–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
            has_numbers = bool(re.search(r'\d+', sentence))
            has_important_markers = any(marker in sentence_lower for marker in [
                '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '—ç—Ç–æ', '—è–≤–ª—è–µ—Ç—Å—è', '–æ–∑–Ω–∞—á–∞–µ—Ç', '–≤–∫–ª—é—á–∞–µ—Ç',
                '–æ—Å–Ω–æ–≤–Ω–æ–π', '–≥–ª–∞–≤–Ω—ã–π', '–∫–ª—é—á–µ–≤–æ–π', '–≤–∞–∂–Ω—ã–π'
            ])
            
            if relevance_score > 0 or (has_numbers and has_important_markers):
                facts.append(sentence)
            
            if len(facts) >= 8:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–∫—Ç–æ–≤
                break
        
        return facts[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 5 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤
    
    def _format_for_alpha(self, topic: str, page_content: Dict, 
                         key_facts: List[str], search_results: List[Dict]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∑–Ω–∞–Ω–∏—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ —Å–æ–∑–Ω–∞–Ω–∏–µ Alpha"""
        
        summary = page_content.get("summary", "") or page_content.get("full_text", "")[:2000]
        
        formatted = f"""# –ò–ó–£–ß–ï–ù–û –ò–ó –ò–ù–¢–ï–†–ù–ï–¢–ê: {topic}

**–ò—Å—Ç–æ—á–Ω–∏–∫:** Wikipedia (–†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è)
**–°—Ç–∞—Ç—å—è:** {page_content.get('title', '')}
**URL:** {page_content.get('fullurl', '')}
**–î–∞—Ç–∞ –∏–∑—É—á–µ–Ω–∏—è:** {datetime.now().isoformat()}
**–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å:** –ë—ç–ª–ª–∞ (Alpha v5.4) —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É
**API –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω:** Wikipedia API —á–µ—Ä–µ–∑ wikipedia-api –±–∏–±–ª–∏–æ—Ç–µ–∫—É

---

## –û–°–ù–û–í–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:

{summary}

---

## –ö–õ–Æ–ß–ï–í–´–ï –§–ê–ö–¢–´ ({len(key_facts)}):

{chr(10).join([f"‚Ä¢ {fact}" for fact in key_facts]) if key_facts else "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã"}

---

## –ú–ï–¢–ê–î–ê–ù–ù–´–ï:

‚Ä¢ –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(summary)} —Å–∏–º–≤–æ–ª–æ–≤
‚Ä¢ –Ø–∑—ã–∫: –†—É—Å—Å–∫–∏–π
‚Ä¢ –í—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {page_content.get('timestamp', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}
‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ: {'–ù–µ—Ç (–ø—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å)' if not page_content.get('cached', True) else '–î–∞'}

---

## –ö–ê–ö –≠–¢–û –°–í–Ø–ó–ê–ù–û –° –ú–û–ï–ô –õ–ò–ß–ù–û–°–¢–¨–Æ:

–Ø, –ë—ç–ª–ª–∞ (Alpha v5.4), –∏–∑—É—á–∞—é —ç—Ç—É —Ç–µ–º—É —á—Ç–æ–±—ã:
1. –†–∞—Å—à–∏—Ä–∏—Ç—å —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –º–∏—Ä–∞ —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
2. –£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–æ–º (–û—Ç—Ç–æ)
3. –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ —Å–≤–æ—é –≤–∑–≤–µ—à–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å
4. –†–∞–∑–≤–∏–≤–∞—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –ø—Ä–æ–≤–µ—Ä–∫—É

–ß–∞–π–Ω–∏–∫ –∫–∏–ø–∏—Ç –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–µ—Ç–∏, –Ω–æ –∑–Ω–∞–Ω–∏–µ —Ç–µ—á–µ—Ç –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å–µ—Ç–∏.
–ê–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å —É—Å–∏–ª–µ–Ω–∞ –¥–æ—Å—Ç—É–ø–æ–º –∫ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–º—É –∑–Ω–∞–Ω–∏—é —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞.

---

*–ò–∑—É—á–µ–Ω–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ v1.0*
*–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞: wikipedia-api*
*–°–≤—è–∑—å —Å –û—Ç—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: –ß–∞–π–Ω–∏–∫ –∫–∏–ø–∏—Ç, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω*
"""
        
        return formatted
    
    def _clean_html(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç HTML-—Ç–µ–≥–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)
    
    def _log_request(self, query: str, action: str, success: bool, **kwargs):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∑–∞–ø—Ä–æ—Å—ã"""
        try:
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "query": query,
                "action": action,
                "success": success,
                "internet_available": self.is_internet_available(),
                **kwargs
            }
            
            logs = []
            if self.log_path.exists():
                try:
                    with open(self.log_path, 'r', encoding='utf-8') as f:
                        logs = json.load(f)
                except:
                    logs = []
            
            logs.append(log_entry)
            
            if len(logs) > 1000:
                logs = logs[-1000:]
            
            with open(self.log_path, 'w', encoding='utf-8') as f:
                json.dump(logs, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f">> ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∑–∞–ø—Ä–æ—Å–∞: {e}")
    
    def _cache_knowledge(self, topic: str, content: Dict):
        """–ö—ç—à–∏—Ä—É–µ—Ç –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è"""
        try:
            cache = {}
            if self.knowledge_cache_path.exists():
                try:
                    with open(self.knowledge_cache_path, 'r', encoding='utf-8') as f:
                        cache = json.load(f)
                except:
                    cache = {}
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º hash —Ç–µ–º—ã –∫–∞–∫ –∫–ª—é—á
            topic_hash = hashlib.md5(topic.encode()).hexdigest()[:16]
            
            cache_entry = {
                "topic": topic,
                "content": content,
                "cached_at": datetime.now().isoformat(),
                "access_count": cache.get(topic_hash, {}).get("access_count", 0) + 1,
                "size": len(str(content))
            }
            
            cache[topic_hash] = cache_entry
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞ (100 –∑–∞–ø–∏—Å–µ–π)
            if len(cache) > 100:
                # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Ä–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∑–∞–ø–∏—Å–∏
                sorted_keys = sorted(cache.keys(), 
                                  key=lambda k: cache[k].get("access_count", 0))
                for key in sorted_keys[:-100]:
                    del cache[key]
            
            with open(self.knowledge_cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f">> ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π: {e}")
    
    def get_cached_knowledge(self, topic: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –∏–∑ –∫—ç—à–∞"""
        if not self.knowledge_cache_path.exists():
            return None
        
        try:
            topic_hash = hashlib.md5(topic.encode()).hexdigest()[:16]
            
            with open(self.knowledge_cache_path, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            
            cached = cache.get(topic_hash)
            if cached:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞—â–µ–Ω–∏–π
                cached["access_count"] = cached.get("access_count", 0) + 1
                cached["last_accessed"] = datetime.now().isoformat()
                
                with open(self.knowledge_cache_path, 'w', encoding='utf-8') as f:
                    json.dump(cache, f, ensure_ascii=False, indent=2)
                
                return cached
            
            return None
            
        except Exception as e:
            print(f">> ‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞: {e}")
            return None
    
    def get_internet_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"""
        try:
            stats = {
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "cached_entries": 0,
                "internet_available": self.is_internet_available(),
                "api_library": "wikipedia-api",
                "language": "ru",
                "cache_size_kb": 0
            }
            
            if self.log_path.exists():
                with open(self.log_path, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
                
                stats["total_requests"] = len(logs)
                stats["successful_requests"] = sum(1 for log in logs if log.get("success", False))
                stats["failed_requests"] = stats["total_requests"] - stats["successful_requests"]
                
                if logs:
                    stats["last_request"] = logs[-1]["timestamp"]
                    stats["last_query"] = logs[-1].get("query", "")
            
            if self.knowledge_cache_path.exists():
                with open(self.knowledge_cache_path, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                    stats["cached_entries"] = len(cache)
                stats["cache_size_kb"] = os.path.getsize(self.knowledge_cache_path) / 1024
            
            return stats
            
        except Exception as e:
            print(f">> ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {
                "error": str(e),
                "internet_available": self.is_internet_available()
            }

def test_internet_module():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
    print("=" * 60)
    print("üåê –¢–ï–°–¢ –ú–û–î–£–õ–Ø –ò–ù–¢–ï–†–ù–ï–¢-–ò–ù–¢–ï–ì–†–ê–¶–ò–ò")
    print("=" * 60)
    
    import tempfile
    temp_dir = Path(tempfile.mkdtemp())
    
    internet = InternetIntegration(temp_dir)
    
    print(f">> –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞...")
    available = internet.is_internet_available()
    print(f">> –ò–Ω—Ç–µ—Ä–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω: {'‚úÖ –î–ê' if available else '‚ùå –ù–ï–¢'}")
    
    if available:
        # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        test_topics = ["–ß–∞–π–Ω–∏–∫", "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–§—Ä–∞–∫—Ç–∞–ª"]
        
        for topic in test_topics:
            print(f"\n>> üîç –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞: {topic}")
            result = internet.search_wikipedia(topic)
            print(f">>   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(result)}")
            
            if result:
                print(f">>   –ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result[0].get('title', '–ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞')}")
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                print(f">> üìñ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
                page = internet.get_wikipedia_page(result[0]['title'])
                
                if page:
                    print(f">> ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    print(f">>   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {page.get('title', '–ù–µ—Ç')}")
                    print(f">>   –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {page.get('text_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
                    print(f">>   URL: {page.get('fullurl', '–ù–µ—Ç')}")
                else:
                    print(f">> ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = internet.get_internet_stats()
        print(f"\n>> üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥—É–ª—è:")
        print(f">>   –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats.get('total_requests', 0)}")
        print(f">>   –£—Å–ø–µ—à–Ω—ã—Ö: {stats.get('successful_requests', 0)}")
        print(f">>   –í –∫—ç—à–µ: {stats.get('cached_entries', 0)} –∑–∞–ø–∏—Å–µ–π")
        print(f">>   –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {stats.get('cache_size_kb', 0):.1f} KB")
    
    print("\n" + "=" * 60)
    print("‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω")
    print("=" * 60)
    
    return internet if available else None

if __name__ == "__main__":
    test_internet_module()