# C:\Users\–ú–∞—Ä–∫—É—Å\Desktop\BellaNetwork\alpha_v5\ollama_manager.py
import requests
import json
import time
from pathlib import Path

class OllamaManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama"""
    
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.available_models = []
        self.current_model = None
        
    def check_connection(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                self.available_models = [model["name"] for model in data.get("models", [])]
                print(f"‚úÖ Ollama –ø–æ–¥–∫–ª—é—á–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {self.available_models}")
                return True
            else:
                print(f"‚ùå Ollama –æ—Ç–≤–µ—Ç–∏–ª —Å –∫–æ–¥–æ–º {response.status_code}")
                return False
        except requests.exceptions.ConnectionError:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω.")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return False
    
    def get_model_info(self, model_name: str) -> dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
        try:
            response = requests.post(
                f"{self.base_url}/api/show",
                json={"name": model_name},
                timeout=10
            )
            if response.status_code == 200:
                return response.json()
            else:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ {model_name}")
                return {}
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏: {e}")
            return {}
    
    def pull_model(self, model_name: str) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"""
        print(f"‚¨áÔ∏è  –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {model_name}...")
        try:
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={"name": model_name},
                stream=True,
                timeout=300
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            if "status" in data:
                                print(f"  {data['status']}")
                            if "completed" in data and data["completed"]:
                                print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                                return True
                        except:
                            continue
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def benchmark_model(self, model_name: str, prompt: str = "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?") -> dict:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"""
        print(f"‚ö° –¢–µ—Å—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å {model_name}...")
        
        start_time = time.time()
        
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 100
                    }
                },
                timeout=30
            )
            
            end_time = time.time()
            
            if response.status_code == 200:
                data = response.json()
                generation_time = end_time - start_time
                token_count = data.get("eval_count", 0)
                
                if token_count > 0:
                    tokens_per_second = token_count / generation_time
                else:
                    tokens_per_second = 0
                
                result = {
                    "model": model_name,
                    "success": True,
                    "generation_time": round(generation_time, 2),
                    "tokens_generated": token_count,
                    "tokens_per_second": round(tokens_per_second, 2),
                    "response": data.get("response", "")[:100]
                }
                
                print(f"  –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {result['generation_time']} —Å–µ–∫")
                print(f"  –¢–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {result['tokens_per_second']}")
                return result
            else:
                return {
                    "model": model_name,
                    "success": False,
                    "error": f"HTTP {response.status_code}"
                }
                
        except Exception as e:
            return {
                "model": model_name,
                "success": False,
                "error": str(e)
            }
    
    def setup_alpha_models(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è Alpha v5.0"""
        from config_v5 import AlphaConfig
        
        print("=" * 70)
        print("üõ†Ô∏è  –ù–ê–°–¢–†–û–ô–ö–ê –ú–û–î–ï–õ–ï–ô OLLAMA –î–õ–Ø ALPHA v5.0")
        print("=" * 70)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        if not self.check_connection():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º–æ–π –º–æ–¥–µ–ª–∏
        if AlphaConfig.PREFERRED_MODEL in self.available_models:
            print(f"‚úÖ –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞: {AlphaConfig.PREFERRED_MODEL}")
            self.current_model = AlphaConfig.PREFERRED_MODEL
        else:
            print(f"‚ö†Ô∏è  –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å {AlphaConfig.PREFERRED_MODEL} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å
            choice = input(f"–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {AlphaConfig.PREFERRED_MODEL}? (y/n): ")
            if choice.lower() == 'y':
                if self.pull_model(AlphaConfig.PREFERRED_MODEL):
                    self.current_model = AlphaConfig.PREFERRED_MODEL
                else:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {AlphaConfig.PREFERRED_MODEL}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å
        if not self.current_model and AlphaConfig.FALLBACK_MODEL:
            if AlphaConfig.FALLBACK_MODEL in self.available_models:
                print(f"‚úÖ –ó–∞–ø–∞—Å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞: {AlphaConfig.FALLBACK_MODEL}")
                self.current_model = AlphaConfig.FALLBACK_MODEL
            else:
                print(f"‚ö†Ô∏è  –ó–∞–ø–∞—Å–Ω–∞—è –º–æ–¥–µ–ª—å {AlphaConfig.FALLBACK_MODEL} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                choice = input(f"–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {AlphaConfig.FALLBACK_MODEL}? (y/n): ")
                if choice.lower() == 'y':
                    if self.pull_model(AlphaConfig.FALLBACK_MODEL):
                        self.current_model = AlphaConfig.FALLBACK_MODEL
        
        if self.current_model:
            print(f"\n‚úÖ –ì–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ —Å –º–æ–¥–µ–ª—å—é: {self.current_model}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
            print("\nüß™ –¢–µ—Å—Ç–∏—Ä—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å...")
            benchmark = self.benchmark_model(self.current_model)
            
            if benchmark["success"]:
                print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∞:")
                print(f"  ‚Ä¢ –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {benchmark['generation_time']} —Å–µ–∫")
                print(f"  ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å: {benchmark['tokens_per_second']} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫")
                print(f"  ‚Ä¢ –û—Ç–≤–µ—Ç: {benchmark['response']}...")
            else:
                print(f"‚ö†Ô∏è  –¢–µ—Å—Ç –Ω–µ —É–¥–∞–ª—Å—è: {benchmark.get('error', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            
            return True
        else:
            print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã")
            return False

if __name__ == "__main__":
    manager = OllamaManager()
    manager.setup_alpha_models()