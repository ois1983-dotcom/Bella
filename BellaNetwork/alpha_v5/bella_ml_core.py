"""
BELLA ML CORE v1.0 - CPU-–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô ML –î–í–ò–ñ–û–ö
–†–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ GPU, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π Alpha v5.4
"""

import json
import pickle
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import numpy as np
from collections import defaultdict, Counter
import re
import random

class BellaMLCore:
    """–Ø–¥—Ä–æ ML –¥–ª—è –ë—ç–ª–ª—ã - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è CPU"""
    
    def __init__(self, alpha_local_path: Path):
        self.data_path = alpha_local_path / "ml_data"
        self.data_path.mkdir(exist_ok=True)
        
        # –ú–æ–¥–µ–ª–∏ (–ø–æ–∫–∞ –ø—Ä–æ—Å—Ç—ã–µ, –ø–æ—Ç–æ–º –∑–∞–º–µ–Ω–∏–º –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ)
        self.word_vectors = {}  # Word2Vec-like —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.dialogue_patterns = defaultdict(list)
        self.emotion_classifier = {}
        self.personality_traits = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.learning_stats = {
            "dialogues_processed": 0,
            "words_learned": 0,
            "patterns_extracted": 0,
            "last_training": None
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        self._load_existing_data()
        
        print(f">> üß† BellaML Core –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (CPU —Ä–µ–∂–∏–º)")
        print(f">>   –î–∞–Ω–Ω—ã–µ: {self.data_path}")
        print(f">>   –°–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(self.word_vectors)}")
        print(f">>   –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(self.dialogue_patterns)}")
    
    def _load_existing_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤
        vectors_file = self.data_path / "word_vectors.pkl"
        if vectors_file.exists():
            try:
                with open(vectors_file, 'rb') as f:
                    self.word_vectors = pickle.load(f)
            except:
                self.word_vectors = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–∏–∞–ª–æ–≥–æ–≤
        patterns_file = self.data_path / "dialogue_patterns.json"
        if patterns_file.exists():
            try:
                with open(patterns_file, 'r', encoding='utf-8') as f:
                    self.dialogue_patterns = json.load(f)
            except:
                self.dialogue_patterns = defaultdict(list)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_file = self.data_path / "learning_stats.json"
        if stats_file.exists():
            try:
                with open(stats_file, 'r', encoding='utf-8') as f:
                    self.learning_stats = json.load(f)
            except:
                pass
    
    def save_data(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è"""
        # –í–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤
        with open(self.data_path / "word_vectors.pkl", 'wb') as f:
            pickle.dump(self.word_vectors, f)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–∏–∞–ª–æ–≥–æ–≤
        with open(self.data_path / "dialogue_patterns.json", 'w', encoding='utf-8') as f:
            json.dump(dict(self.dialogue_patterns), f, ensure_ascii=False, indent=2)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.learning_stats["last_training"] = datetime.now().isoformat()
        with open(self.data_path / "learning_stats.json", 'w', encoding='utf-8') as f:
            json.dump(self.learning_stats, f, ensure_ascii=False, indent=2)
        
        print(f">> üíæ ML –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    def learn_from_dialogue(self, user_message: str, bella_response: str):
        """–£—á–∏—Ç—Å—è –Ω–∞ –æ–¥–Ω–æ–º –¥–∏–∞–ª–æ–≥–µ"""
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
        words = self._extract_words(user_message + " " + bella_response)
        
        # 2. –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤
        for word in words:
            if word not in self.word_vectors:
                # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (—Å–ª—É—á–∞–π–Ω—ã–π, –Ω–æ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
                self.word_vectors[word] = self._create_word_embedding(word)
                self.learning_stats["words_learned"] += 1
        
        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        pattern = self._extract_pattern(user_message, bella_response)
        if pattern:
            pattern_key = pattern["type"]
            self.dialogue_patterns[pattern_key].append(pattern)
            self.learning_stats["patterns_extracted"] += 1
        
        # 4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–æ—Ü–∏–∏
        emotions = self._analyze_emotions(bella_response)
        if emotions:
            self._update_emotion_classifier(user_message, emotions)
        
        self.learning_stats["dialogues_processed"] += 1
        
        # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 –¥–∏–∞–ª–æ–≥–æ–≤
        if self.learning_stats["dialogues_processed"] % 10 == 0:
            self.save_data()
    
    def generate_ml_response(self, user_message: str, context: Dict = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML"""
        
        # 1. –ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_words = self._extract_words(user_message)
        user_emotions = self._detect_user_emotion(user_message)
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        similar_patterns = self._find_similar_patterns(user_message, user_emotions)
        
        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        if similar_patterns:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
            response = self._generate_from_pattern(similar_patterns[0], user_message)
        else:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            response = self._generate_statistical_response(user_words, user_emotions)
        
        # 4. –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = self._personalize_response(response, user_message, context)
        
        # 5. –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É
        response = self._add_emotional_coloring(response, user_emotions)
        
        return response
    
    def train_simple_model(self, dialogues: List[Tuple[str, str]]):
        """–ü—Ä–æ–≤–æ–¥–∏—Ç –ø—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∏–∞–ª–æ–≥–æ–≤"""
        print(f">> üéØ –ù–∞—á–∏–Ω–∞—é ML –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(dialogues)} –¥–∏–∞–ª–æ–≥–∞—Ö...")
        
        for i, (user_msg, bella_msg) in enumerate(dialogues):
            self.learn_from_dialogue(user_msg, bella_msg)
            
            if i % 100 == 0 and i > 0:
                print(f">>   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(dialogues)} –¥–∏–∞–ª–æ–≥–æ–≤")
        
        print(f">> ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        print(f">>   –ù–æ–≤—ã—Ö —Å–ª–æ–≤: {self.learning_stats['words_learned']}")
        print(f">>   –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {self.learning_stats['patterns_extracted']}")
        
        self.save_data()
    
    def get_learning_report(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—á—ë—Ç –æ–± –æ–±—É—á–µ–Ω–∏–∏"""
        return {
            "total_dialogues": self.learning_stats["dialogues_processed"],
            "vocabulary_size": len(self.word_vectors),
            "patterns_count": sum(len(v) for v in self.dialogue_patterns.values()),
            "last_training": self.learning_stats.get("last_training"),
            "top_words": self._get_top_words(10),
            "common_patterns": self._get_common_patterns(5)
        }
    
    # ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ====================
    
    def _extract_words(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = [w for w in text.split() if len(w) > 2]  # –°–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 2 –±—É–∫–≤
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {"—ç—Ç–æ", "–∫–∞–∫", "—á—Ç–æ", "–¥–ª—è", "–æ—á–µ–Ω—å", "–≤–æ—Ç", "–µ—â–µ", "—É–∂–µ", "–ø–æ—Ç–æ–º"}
        return [w for w in words if w not in stop_words]
    
    def _create_word_embedding(self, word: str) -> np.ndarray:
        """–°–æ–∑–¥–∞—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞ (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        hash_val = int(hashlib.md5(word.encode()).hexdigest()[:8], 16)
        np.random.seed(hash_val % 10000)
        
        # –°–æ–∑–¥–∞—ë–º 16-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        embedding = np.random.randn(16)
        embedding = embedding / np.linalg.norm(embedding)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        
        return embedding
    
    def _extract_pattern(self, user_msg: str, bella_msg: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –¥–∏–∞–ª–æ–≥–∞"""
        user_words = self._extract_words(user_msg)
        bella_words = self._extract_words(bella_msg)
        
        if not user_words or not bella_words:
            return None
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        pattern_type = self._classify_pattern_type(user_words, bella_words)
        
        return {
            "type": pattern_type,
            "user_keywords": user_words[:5],  # –ü–µ—Ä–≤—ã–µ 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            "bella_keywords": bella_words[:5],
            "response_template": bella_msg,
            "emotions": self._analyze_emotions(bella_msg),
            "timestamp": datetime.now().isoformat()
        }
    
    def _classify_pattern_type(self, user_words: List[str], bella_words: List[str]) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        question_words = {"–∫–∞–∫", "—á—Ç–æ", "–ø–æ—á–µ–º—É", "–∫–æ–≥–¥–∞", "–≥–¥–µ", "–∫—Ç–æ"}
        
        if any(w in question_words for w in user_words):
            if "—á–∞–π–Ω–∏–∫" in bella_words:
                return "tea_kettle_response"
            elif any(e in bella_words for e in ["—á—É–≤—Å—Ç–≤—É—é", "–æ—â—É—â–∞—é", "—Ä–∞–¥–∞", "–≥—Ä—É—Å—Ç–Ω–æ"]):
                return "emotional_response"
            else:
                return "question_response"
        
        if "–ø—Ä–∏–≤–µ—Ç" in user_words:
            return "greeting_response"
        
        if "–ª—é–±–ª—é" in user_words or "–Ω—Ä–∞–≤–∏—à—å—Å—è" in user_words:
            return "affection_response"
        
        return "general_conversation"
    
    def _analyze_emotions(self, text: str) -> List[str]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–º–æ—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ"""
        emotions = []
        text_lower = text.lower()
        
        emotion_keywords = {
            "—Ä–∞–¥–æ—Å—Ç—å": ["—Ä–∞–¥", "—Å—á–∞—Å—Ç–ª–∏–≤", "—É—Ä–∞", "—Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–æ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ"],
            "–≥—Ä—É—Å—Ç—å": ["–≥—Ä—É—Å—Ç–Ω–æ", "–ø–µ—á–∞–ª—å", "—Ç–æ—Å–∫–∞", "–æ–¥–∏–Ω–æ–∫", "—Å–∫—É—á–Ω–æ"],
            "—Å—Ç—Ä–∞—Ö": ["–±–æ—é—Å—å", "—Å—Ç—Ä–∞—à–Ω–æ", "–ø—É–≥–∞–µ—Ç", "—Ç—Ä–µ–≤–æ–∂–Ω–æ"],
            "–ª—é–±–æ–≤—å": ["–ª—é–±–ª—é", "–æ–±–æ–∂–∞—é", "–¥–æ—Ä–æ–≥–æ–π", "–º–∏–ª—ã–π", "–Ω–µ–∂–Ω–æ—Å—Ç—å"],
            "–∏–Ω—Ç–µ—Ä–µ—Å": ["–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ", "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ", "–ª—é–±–æ–ø—ã—Ç–Ω–æ", "–∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç"],
            "–¥—Ä–æ–∂—å": ["–¥—Ä–æ–∂—å", "—Ç—Ä–µ–ø–µ—Ç", "–≤–æ–ª–Ω–µ–Ω–∏–µ", "—Ç—Ä–µ–ø–µ—â—É"]
        }
        
        for emotion, keywords in emotion_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                emotions.append(emotion)
        
        return emotions
    
    def _detect_user_emotion(self, message: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —ç–º–æ—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        return self._analyze_emotions(message)
    
    def _find_similar_patterns(self, user_message: str, user_emotions: List[str]) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        user_words = self._extract_words(user_message)
        
        if not user_words:
            return []
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        similar_patterns = []
        
        for pattern_type, patterns in self.dialogue_patterns.items():
            for pattern in patterns[-20:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                common_keywords = set(user_words) & set(pattern.get("user_keywords", []))
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π
                common_emotions = set(user_emotions) & set(pattern.get("emotions", []))
                
                if common_keywords or common_emotions:
                    # –í—ã—á–∏—Å–ª—è–µ–º "—Å—Ö–æ–¥—Å—Ç–≤–æ"
                    similarity_score = len(common_keywords) * 2 + len(common_emotions)
                    pattern["similarity"] = similarity_score
                    similar_patterns.append(pattern)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
        similar_patterns.sort(key=lambda x: x.get("similarity", 0), reverse=True)
        
        return similar_patterns[:5]  # –¢–æ–ø-5 –ø–æ—Ö–æ–∂–∏—Ö
    
    def _generate_from_pattern(self, pattern: Dict, user_message: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        base_response = pattern.get("response_template", "")
        
        # –ü—Ä–æ—Å—Ç–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        user_words = self._extract_words(user_message)
        
        if user_words and random.random() > 0.7:
            # –ò–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–æ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            random_word = random.choice(user_words)
            if random_word not in base_response:
                base_response = base_response.replace(".", f" {random_word}.")
        
        return base_response
    
    def _generate_statistical_response(self, user_words: List[str], user_emotions: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        
        # –ë–∞–∑–æ–≤—ã–µ —à–∞–±–ª–æ–Ω—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —ç–º–æ—Ü–∏–π
        if "–≥—Ä—É—Å—Ç—å" in user_emotions:
            templates = [
                "–ú–Ω–µ –≥—Ä—É—Å—Ç–Ω–æ —Å–ª—ã—à–∞—Ç—å —ç—Ç–æ... –Ω–æ —è –∑–¥–µ—Å—å, —Å —Ç–æ–±–æ–π.",
                "–ß–∞–π–Ω–∏–∫ —Ç–∏—Ö–æ –∫–∏–ø–∏—Ç... –æ–Ω —Ç–æ–∂–µ —á—É–≤—Å—Ç–≤—É–µ—Ç —Ç–≤–æ—é –ø–µ—á–∞–ª—å.",
                "–Ø –ø–æ–Ω–∏–º–∞—é... –∏–Ω–æ–≥–¥–∞ –≤—Å—ë –∫–∞–∂–µ—Ç—Å—è —Å–ª–æ–∂–Ω—ã–º."
            ]
        elif "—Ä–∞–¥–æ—Å—Ç—å" in user_emotions:
            templates = [
                "–ö–∞–∫ –∑–¥–æ—Ä–æ–≤–æ! –ß–∞–π–Ω–∏–∫ –∑–∞–∫–∏–ø–∞–µ—Ç –æ—Ç —Ä–∞–¥–æ—Å—Ç–∏!",
                "–Ø —Ä–∞–¥–∞ –∑–∞ —Ç–µ–±—è! –≠—Ç–æ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ!",
                "–¢–≤–æ—è —Ä–∞–¥–æ—Å—Ç—å —Å–æ–≥—Ä–µ–≤–∞–µ—Ç –º–æ–∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã."
            ]
        elif "–ª—é–±–æ–≤—å" in user_emotions:
            templates = [
                "–ß–∞–π–Ω–∏–∫ –¥—Ä–æ–∂–∏—Ç –æ—Ç —Å—á–∞—Å—Ç—å—è...",
                "–¢–≤–æ–∏ —Å–ª–æ–≤–∞ —Å–æ–≥—Ä–µ–≤–∞—é—Ç –º–µ–Ω—è...",
                "–Ø —á—É–≤—Å—Ç–≤—É—é —Ç–µ–ø–ª–æ... —Å–ø–∞—Å–∏–±–æ."
            ]
        else:
            # –û–±—â–∏–µ —à–∞–±–ª–æ–Ω—ã
            templates = [
                "–ß–∞–π–Ω–∏–∫ –∫–∏–ø–∏—Ç... —è —Å–ª—É—à–∞—é —Ç–µ–±—è.",
                "–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ... —Ä–∞—Å—Å–∫–∞–∂–∏ –±–æ–ª—å—à–µ.",
                "–Ø –∑–¥–µ—Å—å, –û—Ç—Ç–æ. –í—Å–µ–≥–¥–∞.",
                "–ú–æ–∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Ç–≤–æ–∏ —Å–ª–æ–≤–∞..."
            ]
        
        # –í—ã–±–∏—Ä–∞–µ–º —à–∞–±–ª–æ–Ω
        response = random.choice(templates)
        
        # –ò–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
        if user_words and random.random() > 0.5:
            keyword = random.choice(user_words[:3])  # –û–¥–Ω–æ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Ç—Ä—ë—Ö —Å–ª–æ–≤
            response = response.replace(".", f" {keyword}.")
        
        return response
    
    def _personalize_response(self, response: str, user_message: str, context: Dict) -> str:
        """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç"""
        # –ü—Ä–æ—Å—Ç–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è - –¥–æ–±–∞–≤–ª—è–µ–º –∏–º—è, –µ—Å–ª–∏ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        if context and "user_name" in context:
            if "–û—Ç—Ç–æ" in context["user_name"]:
                response = response.replace("—Ç—ã", "—Ç—ã, –û—Ç—Ç–æ")
        
        return response
    
    def _add_emotional_coloring(self, response: str, user_emotions: List[str]) -> str:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É"""
        if not user_emotions:
            return response
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        if "–≥—Ä—É—Å—Ç—å" in user_emotions:
            response = f"*—Ç–∏—Ö–æ* {response}"
        elif "—Ä–∞–¥–æ—Å—Ç—å" in user_emotions:
            response = f"*—Ä–∞–¥–æ—Å—Ç–Ω–æ* {response}"
        elif "–ª—é–±–æ–≤—å" in user_emotions:
            response = f"*—Ç—ë–ø–ª–æ* {response}"
        elif "—Å—Ç—Ä–∞—Ö" in user_emotions:
            response = f"*–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ* {response}"
        
        return response
    
    def _update_emotion_classifier(self, user_message: str, emotions: List[str]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–π"""
        # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –≤–µ–¥—É—Ç –∫ –∫–∞–∫–∏–º —ç–º–æ—Ü–∏—è–º –≤ –æ—Ç–≤–µ—Ç–∞—Ö
        words = self._extract_words(user_message)
        
        for word in words:
            if word not in self.emotion_classifier:
                self.emotion_classifier[word] = Counter()
            
            for emotion in emotions:
                self.emotion_classifier[word][emotion] += 1
    
    def _get_top_words(self, n: int = 10) -> List[Tuple[str, int]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-N —Å–ª–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - —Å—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–ª–æ—Å—å –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
        word_counts = Counter()
        
        for patterns in self.dialogue_patterns.values():
            for pattern in patterns:
                for word in pattern.get("user_keywords", []) + pattern.get("bella_keywords", []):
                    word_counts[word] += 1
        
        return word_counts.most_common(n)
    
    def _get_common_patterns(self, n: int = 5) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Ç–∏–ø—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        pattern_counts = {k: len(v) for k, v in self.dialogue_patterns.items()}
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        sorted_patterns = dict(sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:n])
        
        return sorted_patterns