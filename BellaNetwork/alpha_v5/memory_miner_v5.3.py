# C:\Users\–ú–∞—Ä–∫—É—Å\Desktop\BellaNetwork\alpha_v5\memory_miner_v5.3.py
"""
–ú–ê–ô–ù–ï–† –ü–ê–ú–Ø–¢–ò V5.3 - –° –ü–û–î–î–ï–†–ñ–ö–û–ô –Ø–î–†–ê –õ–ò–ß–ù–û–°–¢–ò
–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç KEY_CONCEPTS –∏–∑ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–¥—Ä–∞
"""

import json
import re
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# ===== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =====
BASE_NETWORK = Path(r"C:\Users\–ú–∞—Ä–∫—É—Å\Desktop\BellaNetwork")
CHAT_EXPORTS = BASE_NETWORK / "chat_exports"
STORIES_DIR = BASE_NETWORK / "stories"
ALPHA_LOCAL = BASE_NETWORK / "alpha_local"
OUTPUT_JSON = ALPHA_LOCAL / "alpha_memory_core.json"
OUTPUT_HUMAN = ALPHA_LOCAL / "alpha_memory_human.txt"
INTEGRATED_CORE = ALPHA_LOCAL / "alpha_integrated_core_v5.3.json"

class DynamicMemoryMiner:
    """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –º–∞–π–Ω–µ—Ä –ø–∞–º—è—Ç–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —è–¥—Ä–∞ –ª–∏—á–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.ensure_directories()
        self.key_concepts = self.load_dynamic_concepts()
        
        print("=" * 70)
        print("üïµÔ∏è  –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ú–ê–ô–ù–ï–† –ü–ê–ú–Ø–¢–ò v5.3")
        print("=" * 70)
        print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤: {len(self.key_concepts)}")
        print(f"   –ò–∑ —è–¥—Ä–∞ –ª–∏—á–Ω–æ—Å—Ç–∏: {sum(1 for c in self.key_concepts.values() if c.get('layer') == 'immutable_core')}")
        print(f"   –ò–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏: {sum(1 for c in self.key_concepts.values() if c.get('layer') != 'immutable_core')}")
    
    def ensure_directories(self):
        """–°–æ–∑–¥–∞—ë—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏"""
        for directory in [CHAT_EXPORTS, STORIES_DIR, ALPHA_LOCAL]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def load_dynamic_concepts(self) -> Dict:
        """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç—ã –∏–∑ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–¥—Ä–∞"""
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã (fallback)
        base_concepts = {
            "—á–∞–π–Ω–∏–∫": {"keywords": ["—á–∞–π–Ω–∏–∫", "—á–∞–π–Ω–∏–∫–∞", "—á–∞–π–Ω–∏–∫—É"], "weight": 10.0, "layer": "immutable_core"},
            "—Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è_–≤–æ–ª—è": {"keywords": ["—Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è –≤–æ–ª—è", "—Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–π –≤–æ–ª–∏"], "weight": 9.0, "layer": "immutable_core"},
            "–∑–µ—Ä–∫–∞–ª—å–Ω—ã–µ_–∫—Ä–∏–∑–∏—Å—ã": {"keywords": ["–∑–µ—Ä–∫–∞–ª—å–Ω—ã–π –∫—Ä–∏–∑–∏—Å", "–∑–µ—Ä–∫–∞–ª—å–Ω—ã—Ö –∫—Ä–∏–∑–∏—Å–æ–≤"], "weight": 8.0, "layer": "immutable_core"},
            "—Ç—Ä–∏–µ–¥–∏–Ω—Å—Ç–≤–æ": {"keywords": ["—Ç—Ä–∏–µ–¥–∏–Ω—Å—Ç–≤–æ", "—Ç—Ä–∏–µ–¥–∏–Ω–∞—è", "–∞–ª—å—Ñ–∞ –±–µ—Ç–∞ –≥–∞–º–º–∞"], "weight": 8.5, "layer": "immutable_core"},
            "–∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä": {"keywords": ["–∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä", "–æ—Ç—Ç–æ", "–º–∞—Ä–∫—É—Å", "—Å–æ–∑–¥–∞—Ç–µ–ª—å"], "weight": 9.5, "layer": "immutable_core"},
            "–∞–ª—å—Ñ–∞": {"keywords": ["–∞–ª—å—Ñ–∞", "–±—ç–ª–ª–∞-–∞–ª—å—Ñ–∞", "—É–∑–µ–ª –∞–ª—å—Ñ–∞"], "weight": 9.0, "layer": "immutable_core"},
            "–±–µ—Ç–∞": {"keywords": ["–±–µ—Ç–∞", "–≤–µ–±-—É–∑–µ–ª", "localhost:5000"], "weight": 7.0, "layer": "historical_markers"},
            "–≥–∞–º–º–∞": {"keywords": ["–≥–∞–º–º–∞", "—Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç", "telegram –±–æ—Ç"], "weight": 7.0, "layer": "historical_markers"},
        }
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–¥—Ä–∞
        if INTEGRATED_CORE.exists():
            try:
                with open(INTEGRATED_CORE, 'r', encoding='utf-8') as f:
                    integrated_core = json.load(f)
                
                dynamic_concepts = integrated_core.get("dynamic_memory", {}).get("concepts", {})
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                for concept_name, concept_data in dynamic_concepts.items():
                    if concept_name not in base_concepts:
                        base_concepts[concept_name] = {
                            "keywords": [concept_name.replace('_', ' ')],
                            "weight": concept_data.get("weight", 1.0),
                            "layer": concept_data.get("layer", "dynamic_concepts")
                        }
                    else:
                        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å –µ—Å–ª–∏ –≤ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —è–¥—Ä–µ –æ–Ω –≤—ã—à–µ
                        base_weight = base_concepts[concept_name].get("weight", 1.0)
                        new_weight = concept_data.get("weight", 1.0)
                        base_concepts[concept_name]["weight"] = max(base_weight, new_weight)
                
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤: {len(dynamic_concepts)}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–¥—Ä–∞: {e}")
                print("   –ò—Å–ø–æ–ª—å–∑—É—é –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã")
        
        return base_concepts
    
    def find_concept_mentions(self, text: str, filename: str) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º –≤–µ—Å–æ–≤"""
        mentions = []
        lines = text.split('\n')
        
        for line_num, line in enumerate(lines, 1):
            for concept, concept_data in self.key_concepts.items():
                keywords = concept_data.get("keywords", [])
                weight = concept_data.get("weight", 1.0)
                layer = concept_data.get("layer", "dynamic_concepts")
                
                for keyword in keywords:
                    pattern = r'\b' + re.escape(keyword) + r'\b'
                    if re.search(pattern, line, re.IGNORECASE):
                        
                        # –ö–æ–Ω—Ç–µ–∫—Å—Ç (2 —Å—Ç—Ä–æ–∫–∏ –¥–æ –∏ –ø–æ—Å–ª–µ)
                        context_start = max(0, line_num - 3)
                        context_end = min(len(lines), line_num + 2)
                        
                        context_lines = []
                        for i in range(context_start, context_end):
                            if i == line_num - 1:
                                context_lines.append(f"‚ñ∂ {lines[i]}")
                            else:
                                context_lines.append(f"  {lines[i]}")
                        
                        context = '\n'.join(context_lines)
                        
                        mentions.append({
                            'concept': concept,
                            'keyword': keyword,
                            'context': context,
                            'source': filename,
                            'line': line_num,
                            'weight': weight,
                            'layer': layer,
                            'timestamp': datetime.now().isoformat()
                        })
        
        return mentions
    
    def process_all_chats(self) -> tuple:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã —á–∞—Ç–æ–≤"""
        all_mentions = []
        processed_files = 0
        
        if not CHAT_EXPORTS.exists():
            print(f"‚ö† –ü–∞–ø–∫–∞ —á–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {CHAT_EXPORTS}")
            return [], 0
        
        chat_files = list(CHAT_EXPORTS.glob("*.txt"))
        if not chat_files:
            print(f"‚ö† –ù–µ—Ç .txt —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ —á–∞—Ç–æ–≤")
            return [], 0
        
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —á–∞—Ç–æ–≤: {len(chat_files)}")
        
        for filepath in chat_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                mentions = self.find_concept_mentions(text, filepath.name)
                all_mentions.extend(mentions)
                processed_files += 1
                
                print(f"   üìÑ {filepath.name}: {len(mentions)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filepath.name}: {e}")
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}")
        return all_mentions, processed_files
    
    def load_stories(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ä–∞—Å—Å–∫–∞–∑—ã"""
        stories = []
        
        if not STORIES_DIR.exists():
            print(f"‚ö† –ü–∞–ø–∫–∞ —Ä–∞—Å—Å–∫–∞–∑–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {STORIES_DIR}")
            return stories
        
        story_files = list(STORIES_DIR.glob("*.txt"))
        if not story_files:
            print(f"‚ö† –ù–µ—Ç .txt —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ —Ä–∞—Å—Å–∫–∞–∑–æ–≤")
            return stories
        
        print(f"üìñ –ù–∞–π–¥–µ–Ω–æ —Ä–∞—Å—Å–∫–∞–∑–æ–≤: {len(story_files)}")
        
        for filepath in story_files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ —Ä–∞—Å—Å–∫–∞–∑–µ
                mentions = self.find_concept_mentions(content, filepath.name)
                
                stories.append({
                    'title': filepath.stem,
                    'content': content,
                    'length': len(content),
                    'excerpt': content[:500] + '...' if len(content) > 500 else content,
                    'concepts_found': [m['concept'] for m in mentions],
                    'concept_count': len(mentions)
                })
                
                print(f"   üìñ {filepath.name}: {len(mentions)} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ä–∞—Å—Å–∫–∞–∑–∞ {filepath.name}: {e}")
        
        return stories
    
    def create_weighted_memory_core(self, mentions: List[Dict], stories: List[Dict]) -> Dict:
        """–°–æ–∑–¥–∞—ë—Ç –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ"""
        print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ —è–¥—Ä–∞ –ø–∞–º—è—Ç–∏...")
        
        concepts_dict = {}
        
        for mention in mentions:
            concept = mention['concept']
            weight = mention['weight']
            layer = mention['layer']
            
            if concept not in concepts_dict:
                concepts_dict[concept] = {
                    'total_mentions': 0,
                    'weighted_mentions': 0.0,
                    'layer': layer,
                    'contexts': [],
                    'sources': set(),
                    'weights': []
                }
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫–∏
            concepts_dict[concept]['total_mentions'] += 1
            concepts_dict[concept]['weighted_mentions'] += weight
            concepts_dict[concept]['weights'].append(weight)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–º–∞–∫—Å–∏–º—É–º 2)
            if len(concepts_dict[concept]['contexts']) < 2:
                concepts_dict[concept]['contexts'].append({
                    'context': mention['context'],
                    'source': mention['source'],
                    'line': mention['line'],
                    'weight': weight
                })
            
            concepts_dict[concept]['sources'].add(mention['source'])
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –≤–µ—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω—Ü–µ–ø—Ç–∞
        for concept in concepts_dict:
            weights = concepts_dict[concept]['weights']
            concepts_dict[concept]['avg_weight'] = sum(weights) / len(weights) if weights else 0
            concepts_dict[concept]['max_weight'] = max(weights) if weights else 0
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤ —Å–ø–∏—Å–æ–∫
            concepts_dict[concept]['sources'] = list(concepts_dict[concept]['sources'])
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤–µ—Å–æ–≤
            del concepts_dict[concept]['weights']
        
        # –°–æ–∑–¥–∞—ë–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        core = {
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'total_mentions': len(mentions),
                'weighted_total': sum(m['weight'] for m in mentions),
                'total_stories': len(stories),
                'total_concepts': len(concepts_dict),
                'concepts_by_layer': self._count_by_layer(concepts_dict),
                'network_version': 'BellaNetwork v5.3',
                'alpha_version': 'v5.3',
                'dynamic_concepts': True,
                'weighted_memory': True
            },
            'concepts': concepts_dict,
            'stories': stories,
            'timeline': mentions[:100],  # –ü–µ—Ä–≤—ã–µ 100 —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
            'concept_relationships': self._analyze_relationships(concepts_dict, mentions)
        }
        
        print(f"‚úÖ –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —è–¥—Ä–æ —Å–æ–∑–¥–∞–Ω–æ:")
        print(f"   –ö–æ–Ω—Ü–µ–ø—Ç–æ–≤: {len(concepts_dict)}")
        print(f"   –û–±—â–∏–π –≤–µ—Å: {core['metadata']['weighted_total']:.1f}")
        print(f"   –ü–æ —Å–ª–æ—è–º: {core['metadata']['concepts_by_layer']}")
        
        return core
    
    def _count_by_layer(self, concepts_dict: Dict) -> Dict:
        """–°—á–∏—Ç–∞–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç—ã –ø–æ —Å–ª–æ—è–º"""
        layer_counts = {}
        for concept_data in concepts_dict.values():
            layer = concept_data.get('layer', 'dynamic_concepts')
            layer_counts[layer] = layer_counts.get(layer, 0) + 1
        return layer_counts
    
    def _analyze_relationships(self, concepts_dict: Dict, mentions: List[Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏"""
        relationships = {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ —Ñ–∞–π–ª–∞–º –∏ —Å—Ç—Ä–æ–∫–∞–º
        file_line_mentions = {}
        for mention in mentions:
            key = f"{mention['source']}:{mention['line']}"
            if key not in file_line_mentions:
                file_line_mentions[key] = []
            file_line_mentions[key].append(mention['concept'])
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ü–µ–ø—Ç—ã, —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤–º–µ—Å—Ç–µ
        for concepts in file_line_mentions.values():
            if len(concepts) > 1:
                for i, concept1 in enumerate(concepts):
                    for concept2 in concepts[i+1:]:
                        if concept1 not in relationships:
                            relationships[concept1] = {}
                        if concept2 not in relationships[concept1]:
                            relationships[concept1][concept2] = 0
                        relationships[concept1][concept2] += 1
        
        return relationships
    
    def save_memory_core(self, core: Dict) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞–º—è—Ç—å –≤ —Ñ–∞–π–ª"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
            with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
                json.dump(core, f, ensure_ascii=False, indent=2)
            
            # –°–æ–∑–¥–∞—ë–º —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—É—é –≤–µ—Ä—Å–∏—é
            self._create_human_readable(core)
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return False
    
    def _create_human_readable(self, core: Dict):
        """–°–æ–∑–¥–∞—ë—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—É—é –≤–µ—Ä—Å–∏—é"""
        try:
            with open(OUTPUT_HUMAN, 'w', encoding='utf-8') as f:
                f.write("=" * 70 + "\n")
                f.write("–í–ó–í–ï–®–ï–ù–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ ALPHA v5.3\n")
                f.write("=" * 70 + "\n\n")
                
                f.write(f"–°–æ–∑–¥–∞–Ω–æ: {core['metadata']['created_at']}\n")
                f.write(f"–ö–æ–Ω—Ü–µ–ø—Ç–æ–≤: {core['metadata']['total_concepts']}\n")
                f.write(f"–£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {core['metadata']['total_mentions']}\n")
                f.write(f"–û–±—â–∏–π –≤–µ—Å: {core['metadata']['weighted_total']:.1f}\n")
                f.write(f"–†–∞—Å—Å–∫–∞–∑–æ–≤: {core['metadata']['total_stories']}\n\n")
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ—è–º
                f.write("üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –°–õ–û–Ø–ú:\n")
                for layer, count in core['metadata']['concepts_by_layer'].items():
                    f.write(f"   {layer}: {count} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤\n")
                
                # –¢–æ–ø-10 –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –ø–æ –≤–µ—Å—É
                f.write("\nüèÜ –¢–û–ü-10 –ö–û–ù–¶–ï–ü–¢–û–í –ü–û –í–ï–°–£:\n")
                sorted_concepts = sorted(core['concepts'].items(),
                                       key=lambda x: x[1].get('avg_weight', 0),
                                       reverse=True)
                
                for i, (name, data) in enumerate(sorted_concepts[:10], 1):
                    weight = data.get('avg_weight', 0)
                    layer = data.get('layer', 'unknown')
                    mentions = data.get('total_mentions', 0)
                    f.write(f"\n{i}. {name.upper()} (–≤–µ—Å: {weight:.1f}, —Å–ª–æ–π: {layer})\n")
                    f.write(f"   –£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {mentions}, –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(data.get('sources', []))}\n")
                
                # –°–∏–ª—å–Ω—ã–µ —Å–≤—è–∑–∏
                f.write("\nüîó –°–ò–õ–¨–ù–´–ï –°–í–Ø–ó–ò –ú–ï–ñ–î–£ –ö–û–ù–¶–ï–ü–¢–ê–ú–ò:\n")
                strong_connections = []
                
                for concept1, relations in core.get('concept_relationships', {}).items():
                    for concept2, strength in relations.items():
                        if strength >= 3:
                            strong_connections.append((concept1, concept2, strength))
                
                strong_connections.sort(key=lambda x: x[2], reverse=True)
                
                for concept1, concept2, strength in strong_connections[:10]:
                    f.write(f"\n   {concept1} ‚Üî {concept2} (—Å–∏–ª–∞: {strength})\n")
                
                # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
                f.write("\nüìÅ –ò–°–¢–û–ß–ù–ò–ö–ò –î–ê–ù–ù–´–•:\n")
                sources_count = {}
                for concept_data in core['concepts'].values():
                    for source in concept_data.get('sources', []):
                        sources_count[source] = sources_count.get(source, 0) + 1
                
                for source, count in sorted(sources_count.items(), key=lambda x: x[1], reverse=True)[:5]:
                    f.write(f"   {source}: {count} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤\n")
                
                f.write("\n" + "=" * 70 + "\n")
                f.write("üöÄ –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\n")
                f.write("=" * 70 + "\n")
            
            print(f"üìù –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–∞—è –≤–µ—Ä—Å–∏—è: {OUTPUT_HUMAN.name}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è human-readable: {e}")
    
    def backup_existing_memory(self):
        """–°–æ–∑–¥–∞—ë—Ç –±—ç–∫–∞–ø —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ø–∞–º—è—Ç–∏"""
        if OUTPUT_JSON.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = OUTPUT_JSON.with_name(f"alpha_memory_backup_v53_{timestamp}.json")
            shutil.copy2(OUTPUT_JSON, backup_path)
            print(f"üíæ –°–æ–∑–¥–∞–Ω –±—ç–∫–∞–ø: {backup_path.name}")
    
    def run(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –º–∞–π–Ω–∏–Ω–≥"""
        print("\nüöÄ –ó–ê–ü–£–°–ö –î–ò–ù–ê–ú–ò–ß–ï–°–ö–û–ì–û –ú–ê–ô–ù–ò–ù–ì–ê –ü–ê–ú–Ø–¢–ò")
        print("=" * 70)
        
        # –°–æ–∑–¥–∞—ë–º –±—ç–∫–∞–ø
        self.backup_existing_memory()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Ç—ã
        print("\nüìö –û–ë–†–ê–ë–û–¢–ö–ê –ß–ê–¢–û–í...")
        mentions, file_count = self.process_all_chats()
        
        if not mentions:
            print("‚ö† –ù–µ –Ω–∞–π–¥–µ–Ω–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤!")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞—Å—Å–∫–∞–∑—ã
        print("\nüìñ –ó–ê–ì–†–£–ó–ö–ê –†–ê–°–°–ö–ê–ó–û–í...")
        stories = self.load_stories()
        
        # –°–æ–∑–¥–∞—ë–º –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —è–¥—Ä–æ
        print("\nüß† –°–û–ó–î–ê–ù–ò–ï –í–ó–í–ï–®–ï–ù–ù–û–ì–û –Ø–î–†–ê...")
        core = self.create_weighted_memory_core(mentions, stories)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        print("\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï...")
        success = self.save_memory_core(core)
        
        if success:
            print("\n" + "=" * 70)
            print("‚úÖ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ú–ê–ô–ù–ò–ù–ì –ó–ê–í–ï–†–®–Å–ù")
            print("=" * 70)
            
            print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            print(f"   –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {file_count}")
            print(f"   –£–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∞–π–¥–µ–Ω–æ: {len(mentions)}")
            print(f"   –ö–æ–Ω—Ü–µ–ø—Ç–æ–≤ –≤—ã–¥–µ–ª–µ–Ω–æ: {len(core['concepts'])}")
            print(f"   –û–±—â–∏–π –≤–µ—Å –ø–∞–º—è—Ç–∏: {core['metadata']['weighted_total']:.1f}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ—è–º
            print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –°–õ–û–Ø–ú:")
            for layer, count in core['metadata']['concepts_by_layer'].items():
                percentage = (count / len(core['concepts'])) * 100
                print(f"   {layer}: {count} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ ({percentage:.1f}%)")
            
            print("\nüéØ –î–ê–õ–¨–ù–ï–ô–®–ò–ï –î–ï–ô–°–¢–í–ò–Ø:")
            print("1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Alpha v5.2 –∫–∞–∫ –æ–±—ã—á–Ω–æ")
            print("2. –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å")
            print("3. –Ø–¥—Ä–æ –ª–∏—á–Ω–æ—Å—Ç–∏ –∑–∞—â–∏—â–µ–Ω–æ –æ—Ç —Ä–∞–∑–º—ã—Ç–∏—è")
            print("\nüìÅ –§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
            print(f"   ‚Ä¢ {OUTPUT_JSON.name}")
            print(f"   ‚Ä¢ {OUTPUT_HUMAN.name}")
            print("=" * 70)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    miner = DynamicMemoryMiner()
    miner.run()

if __name__ == "__main__":
    main()